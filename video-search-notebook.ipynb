{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a536992-2ffc-4158-b874-e396ff87dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import torchvision\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "from torchvision.transforms import Compose, Resize, Normalize\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import ipywidgets as widgets\n",
    "from ipyfilechooser import FileChooser\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01f8a32-26ce-408a-bbe7-4787d01d3195",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d364d071-626e-49f1-8138-01946edf226d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LoadVideo(Dataset):\n",
    "    def __init__(self, path, transforms, vid_stride=1):   \n",
    "        \n",
    "        self.transforms = transforms \n",
    "        self.vid_stride = vid_stride\n",
    "        self.cur_frame = 0\n",
    "        self.cap = cv2.VideoCapture(path)\n",
    "        self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT) / self.vid_stride)\n",
    " \n",
    "    def __getitem__(self, index):    \n",
    "        # Read video\n",
    "        # Skip over frames\n",
    "        for _ in range(self.vid_stride):\n",
    "            self.cur_frame += 1\n",
    "            self.cap.grab()\n",
    "        \n",
    "        # Read frame\n",
    "        _, img = self.cap.retrieve()\n",
    "        timestamp = self.cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "\n",
    "\n",
    "        # Convert to PIL\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(np.uint8(img))\n",
    "        \n",
    "        # Apply transforms\n",
    "        img_t = self.transforms(img)  \n",
    "        \n",
    "        return img_t, to_tensor(img), self.cur_frame, timestamp\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_frames\n",
    "    \n",
    "MODELS = {\n",
    "    \"convnext_base - laion400m_s13b_b51k\": (\"convnext_base\", \"laion400m_s13b_b51k\"),\n",
    "    \"convnext_base_w - laion2b_s13b_b82k\": (\n",
    "        \"convnext_base_w\",\n",
    "        \"laion2b_s13b_b82k\",\n",
    "    ),\n",
    "    \"convnext_base_w - laion2b_s13b_b82k_augreg\": (\n",
    "        \"convnext_base_w\",\n",
    "        \"laion2b_s13b_b82k_augreg\",\n",
    "    ),\n",
    "    \"convnext_base_w - laion_aesthetic_s13b_b82k\": (\n",
    "        \"convnext_base_w\",\n",
    "        \"laion_aesthetic_s13b_b82k\",\n",
    "    ),\n",
    "    \"convnext_base_w_320 - laion_aesthetic_s13b_b82k\": (\n",
    "        \"convnext_base_w_320\",\n",
    "        \"laion_aesthetic_s13b_b82k\",\n",
    "    ),\n",
    "    \"convnext_base_w_320 - laion_aesthetic_s13b_b82k_augreg\": (\n",
    "        \"convnext_base_w_320\",\n",
    "        \"laion_aesthetic_s13b_b82k_augreg\",\n",
    "    ),\n",
    "    \"convnext_large_d - laion2b_s26b_b102k_augreg\": (\n",
    "        \"convnext_large_d\",\n",
    "        \"laion2b_s26b_b102k_augreg\",\n",
    "    ),\n",
    "    \"convnext_large_d_320 - laion2b_s29b_b131k_ft\": (\n",
    "        \"convnext_large_d_320\",\n",
    "        \"laion2b_s29b_b131k_ft\",\n",
    "    ),\n",
    "    \"convnext_large_d_320 - laion2b_s29b_b131k_ft_soup\": (\n",
    "        \"convnext_large_d_320\",\n",
    "        \"laion2b_s29b_b131k_ft_soup\",\n",
    "    ),\n",
    "    \"convnext_xxlarge - laion2b_s34b_b82k_augreg\": (\n",
    "        \"convnext_xxlarge\",\n",
    "        \"laion2b_s34b_b82k_augreg\",\n",
    "    ),\n",
    "    \"convnext_xxlarge - laion2b_s34b_b82k_augreg_rewind\": (\n",
    "        \"convnext_xxlarge\",\n",
    "        \"laion2b_s34b_b82k_augreg_rewind\",\n",
    "    ),\n",
    "    \"convnext_xxlarge - laion2b_s34b_b82k_augreg_soup\": (\n",
    "        \"convnext_xxlarge\",\n",
    "        \"laion2b_s34b_b82k_augreg_soup\",\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33a07fb-afbb-4704-87a8-e4d27dd6ea1e",
   "metadata": {},
   "source": [
    "# Select an Input Video and Configure Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0401f5e-a983-454d-9c0b-a20e928d4a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_widget = FileChooser(\n",
    "    path='./', \n",
    "    title=\"Input Video:\",\n",
    ")\n",
    "model_widget = widgets.Dropdown(\n",
    "    options=list(MODELS.keys()),\n",
    "    value=\"convnext_base_w - laion2b_s13b_b82k\",\n",
    "    description='Model:',\n",
    "    disabled=False,\n",
    ")\n",
    "stride_widget = widgets.IntText(\n",
    "    value=4,\n",
    "    min=1,\n",
    "    description='Frame Stride:',\n",
    "    disabled=False\n",
    ")\n",
    "batch_widget = widgets.IntText(\n",
    "    value=4,\n",
    "    min=1,\n",
    "    description='Batch Size:',\n",
    "    disabled=False\n",
    ")\n",
    "crop_widget = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Center Crop',\n",
    "    disabled=False,\n",
    "    indent=True\n",
    ")\n",
    "cuda_widget = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Cuda',\n",
    "    disabled=False,\n",
    "    indent=True\n",
    ")\n",
    "text_query_widget = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Text Search Query',\n",
    "    description='',\n",
    "    disabled=False   \n",
    ")\n",
    "image_query_widget = FileChooser(\n",
    "    path='./', \n",
    "    title=\"Image Search Query:\",\n",
    ")\n",
    "\n",
    "query_widget = widgets.Dropdown(\n",
    "    options=['Text', 'Image'],\n",
    "    value='Text',\n",
    "    description='Query Type:',\n",
    "    disabled=False,\n",
    ")\n",
    "output = widgets.Output()\n",
    "\n",
    "def query_type_handler(change):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        if change.new == \"Text\":\n",
    "            display(text_query_widget)\n",
    "        else:\n",
    "            display(image_query_widget)\n",
    "\n",
    "query_widget.observe(query_type_handler, names=\"value\")\n",
    "\n",
    "display(\n",
    "    video_widget,\n",
    "    model_widget,\n",
    "    query_widget,\n",
    "    output,\n",
    "    stride_widget,\n",
    "    batch_widget,\n",
    "    crop_widget,\n",
    "    cuda_widget,\n",
    ")\n",
    "with output:\n",
    "    display(text_query_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29bb085-10a0-4a97-8149-fffb00ba3c96",
   "metadata": {},
   "source": [
    "# Search Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471c1bbc-f185-435b-adc7-86fa2a1aa39c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Preparing model...\")\n",
    "\n",
    "# Check inputs\n",
    "assert video_widget.selected, \"An input video should be provided\"\n",
    "assert (\n",
    "    text_query_widget.value is not None or image_query_widget.selected is not None\n",
    "), \"A text or image query should be provided\"\n",
    "\n",
    "if cuda_widget.value:\n",
    "    assert torch.cuda.is_available(), \"Selected cuda but cuda is not available\"\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Initialize model\n",
    "name, weights = MODELS[model_widget.value]\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    name, pretrained=weights, device=device\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "# Load video\n",
    "dataset = LoadVideo(video_widget.selected, transforms=preprocess, vid_stride=stride_widget.value)\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=batch_widget.value, shuffle=False, num_workers=0\n",
    ")\n",
    "\n",
    "if not crop_widget.value:\n",
    "    del preprocess.transforms[1]\n",
    "\n",
    "# Get text query features\n",
    "if text_query_widget.value:\n",
    "    # Tokenize search phrase\n",
    "    tokenizer = open_clip.get_tokenizer(name)\n",
    "    text = tokenizer([text_query_widget.value]).to(device)\n",
    "\n",
    "    # Encode text query\n",
    "    with torch.no_grad():\n",
    "        query_features = model.encode_text(text)\n",
    "        query_features /= query_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Get image query features\n",
    "else:\n",
    "    image = preprocess(Image.open(image_query_widget.selected)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        query_features = model.encode_image(image)\n",
    "        query_features /= query_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Encode each frame and compare with query features\n",
    "res = pd.DataFrame(columns=[\"Frame\", \"Timestamp\", \"Similarity\"])\n",
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "for image, orig, frame, timestamp in tqdm(dataloader):\n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        image_features = model.encode_image(image)\n",
    "\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    probs = query_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
    "    probs = probs[0]\n",
    "\n",
    "    # Save frame similarity values\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Frame\": frame.tolist(),\n",
    "            \"Timestamp\": torch.round(timestamp / 1000, decimals=2).tolist(),\n",
    "            \"Similarity\": probs.tolist(),\n",
    "        }\n",
    "    )\n",
    "    res = pd.concat([res, df])\n",
    "\n",
    "# Create plot of similarity values\n",
    "lines = (\n",
    "    alt.Chart(res)\n",
    "    .mark_line(color=\"firebrick\")\n",
    "    .encode(\n",
    "        alt.X(\"Timestamp\", title=\"Timestamp (seconds)\"),\n",
    "        alt.Y(\"Similarity\", scale=alt.Scale(zero=True, domainMax=1)),\n",
    "    )\n",
    ").properties(width=600)\n",
    "\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618d3df2-6bbd-4af5-9a30-a0b422cdf138",
   "metadata": {},
   "source": [
    "# Display All Frames Over Similarity Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2faf19-222d-4206-98aa-a3f1afe9e989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def view_frames(b):\n",
    "    output.clear_output()\n",
    "    with output:\n",
    "        thresh = thresh_widget.value\n",
    "        \n",
    "        assert 0 <= thresh <= 1.0, \"Threshold must be between 0 and 1\" \n",
    "        assert not save_widget.value or (save_widget.value and path_widget.selected is not None), \"Must choose a save directory\"\n",
    "\n",
    "        frames = res.T.values[0]\n",
    "        timestamps = res.T.values[1]\n",
    "        sims = res.T.values[2]\n",
    "\n",
    "\n",
    "        # Find all frames over the threshold\n",
    "        matches = []\n",
    "        for f, t, s in zip(frames, timestamps, sims):\n",
    "            if s > thresh:\n",
    "                matches.append((f, t, s))\n",
    "\n",
    "        # Display frames\n",
    "        cap = cv2.VideoCapture(video_widget.selected)            \n",
    "        for f, t, s in matches:    \n",
    "            # Grab frame from video\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, f-1)\n",
    "            _, img = cap.read()\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Plot frame\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"{t}s - Score={s:.3f}\")\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "            \n",
    "            if save_widget.value:\n",
    "                Image.fromarray(img).save(os.path.join(path_widget.selected, f\"{f}.jpg\"))\n",
    "            \n",
    "def save_handler(change):\n",
    "    if change.new:\n",
    "        with output_save:\n",
    "            display(path_widget)\n",
    "    else:\n",
    "        output_save.clear_output()\n",
    "            \n",
    "            \n",
    "thresh_widget = widgets.FloatText(\n",
    "    value=0.3,\n",
    "    min=0,\n",
    "    max=1,\n",
    "    step=0.01,\n",
    "    description='Threshold:',\n",
    "    readout=True,\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    ")\n",
    "\n",
    "save_widget = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Save Matched Frames',\n",
    "    disabled=False,\n",
    "    indent=True\n",
    ")\n",
    "output_save = widgets.Output()\n",
    "save_widget.observe(save_handler, names=\"value\")\n",
    "\n",
    "path_widget = FileChooser(\n",
    "    path='./', \n",
    "    title=\"Save Directory:\",\n",
    "    show_only_dirs=True,\n",
    ")\n",
    "\n",
    "button = widgets.Button(\n",
    "    description='View Frames',\n",
    "    disabled=False,\n",
    "    button_style='',\n",
    "    tooltip='View Frames',\n",
    "    icon='check'\n",
    ")\n",
    "button.on_click(view_frames)\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "display(thresh_widget, save_widget, output_save, button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d6c1d6-d594-4165-b87c-d8015d5c0d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
